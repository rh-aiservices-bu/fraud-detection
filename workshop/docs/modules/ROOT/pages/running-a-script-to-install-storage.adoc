[id='running-a-script-to-install-storage']
= Running a script to install local object storage buckets and create connections

For convenience, run a script (provided in the following procedure) that automatically completes these tasks:

* Creates a Minio instance in your project.
* Creates two storage buckets in that Minio instance.
* Generates a random user id and password for your Minio instance.
* Creates two connections in your project, one for each bucket and both using the same credentials.
* Installs required network policies for service mesh functionality.

The script is based on a https://ai-on-openshift.io/tools-and-applications/minio/minio/[guide for deploying Minio].

IMPORTANT: The Minio-based Object Storage that the script creates is *not* meant for production usage.

NOTE: If you want to connect to your own storage, see xref:creating-connections-to-storage.adoc[Creating connections to your own S3-compatible object storage].

.Prerequisites

You must know the OpenShift resource name for your data science project so that you run the provided script in the correct project. To get the project's resource name:

In the {productname-short} dashboard, select *Data science projects* and then click the *?* icon next to the project name. A text box appears with information about the project, including its resource name:

image::projects/ds-project-list-resource-hover.png[Project list resource name, 400]


[NOTE]
====
The following procedure describes how to run the script from the OpenShift console. If you are knowledgeable in OpenShift and can access the cluster from the command line, instead of following the steps in this procedure, you can use the following command to run the script:

----
oc apply -n <your-project-name/> -f https://github.com/rh-aiservices-bu/fraud-detection/raw/main/setup/setup-s3.yaml
----
====

.Procedure

. In the {productname-short} dashboard, click the application launcher icon and then select the *OpenShift Console* option.
+
image::projects/ds-project-ocp-link.png[OpenShift Console Link, 600]

. In the OpenShift console, click *+* in the top navigation bar.
+
image::projects/ocp-console-add-icon.png[Add resources Icon]

. Select your project from the list of projects.
+
image::projects/ocp-console-select-project.png[Select a project, 200]

. Verify that you selected the correct project.
+
image::projects/ocp-console-project-selected.png[Selected project, 200]

. Copy the following code and paste it into the *Import YAML* editor.
+
NOTE: This code gets and applies the `setup-s3-no-sa.yaml` file.
+
[.lines_space]
[.console-input]
[source, yaml]
----
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: demo-setup
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: demo-setup-edit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
  - kind: ServiceAccount
    name: demo-setup
---
apiVersion: batch/v1
kind: Job
metadata:
  name: create-s3-storage
spec:
  selector: {}
  template:
    spec:
      containers:
        - args:
            - -ec
            - |-
              echo -n 'Setting up Minio instance and connections'
              oc apply -f https://github.com/rh-aiservices-bu/fraud-detection/raw/main/setup/setup-s3-no-sa.yaml
          command:
            - /bin/bash
          image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest
          imagePullPolicy: IfNotPresent
          name: create-s3-storage
      restartPolicy: Never
      serviceAccount: demo-setup
      serviceAccountName: demo-setup
----

. Click *Create*.

.Verification

. In the OpenShift console, you should see a "Resources successfully created" message and the following resources listed:
+
* `demo-setup`
* `demo-setup-edit`
* `create-s3-storage`

.  In the {productname-short} dashboard:

.. Select *Data science projects* and then click the name of your project, *Fraud detection*.
.. Click *Connections*. You should see two connections listed: `My Storage` and `Pipeline Artifacts`.
+ 
image::projects/ds-project-connections.png[Connections for Fraud Detection]


[IMPORTANT]
====
If your cluster uses self-signed certificates, your {productname-short} administrator might need to provide a certificate authority (CA) to securely connect to the S3 object storage, as described in link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/latest/html/installing_and_uninstalling_openshift_ai_self-managed/working-with-certificates_certs#accessing-s3-compatible-object-storage-with-self-signed-certificates_certs[Accessing S3-compatible object storage with self-signed certificates] (Self-Managed) or link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service/working-with-certificates_certs#accessing-s3-compatible-object-storage-with-self-signed-certificates_certs[Accessing S3-compatible object storage with self-signed certificates] (Cloud Service).
====

.Next step

If you want to complete the pipelines section of this {deliverable}, go to xref:enabling-data-science-pipelines.adoc[Enabling data science pipelines].

Otherwise, skip to xref:creating-a-workbench.adoc[Creating a workbench].