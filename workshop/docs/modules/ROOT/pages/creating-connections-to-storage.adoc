[id='creating-connections-to-storage']
= Creating connections to your own S3-compatible object storage

If you have existing S3-compatible storage buckets that you want to use for this {deliverable}, you must create a connection to one storage bucket for saving your data and models. If you want to complete the pipelines section of this {deliverable}, create another connection to a different storage bucket for saving pipeline artifacts.

NOTE: If you do not have your own s3-compatible storage, or if you want to use a disposable local MinIO instance instead, skip this section and follow the steps in xref:running-a-script-to-install-storage.adoc[Running a script to install local object storage buckets and create connections].

The provided script automatically completes the following tasks for you: 

* Creates a MinIO instance in your project
* Creates two storage buckets in that MinIO instance
* Creates two connections in your project, one for each bucket with the same credentials
* Installs required network policies for service mesh functionality

.Prerequisites

To create connections to your existing S3-compatible storage buckets, you need the following credential information for the storage buckets:

* Endpoint URL
* Access key
* Secret key
* Region
* Bucket name

If you don't have this information, contact your storage administrator.

.Procedure

. Create a connection for saving your data and models:

.. In the {productname-short} dashboard, navigate to the page for your data science project.

.. Click the *Connections* tab, and then click *Create connection*.

.. In the *Add connection* modal, for the *Connection type* select *S3 compatible object storage - v1*.

.. Complete the *Add connection* form and name your connection *My Storage*. This connection saves your work, including data and models. 

.. Click *Create*.

. Create a connection for saving pipeline artifacts:
+
NOTE: If you do not intend to complete the pipelines section of the {deliverable}, you can skip this step.

.. Click *Add connection*.

.. Complete the form and name your connection *Pipeline Artifacts*.

.. Click *Create*.

.Verification

In the *Connections* tab for the project, check to see that your connections are listed.

[IMPORTANT]
====
If your cluster uses self-signed certificates, your {productname-short} administrator might need to provide a certificate authority (CA) to securely connect to the S3 object storage, as described in link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/latest/html/installing_and_uninstalling_openshift_ai_self-managed/working-with-certificates_certs#accessing-s3-compatible-object-storage-with-self-signed-certificates_certs[Accessing S3-compatible object storage with self-signed certificates^] (Self-Managed) or link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service/working-with-certificates_certs#accessing-s3-compatible-object-storage-with-self-signed-certificates_certs[Accessing S3-compatible object storage with self-signed certificates^] (Cloud Service).
====

.Next step

If you want to complete the pipelines section of this {deliverable}, go to xref:enabling-data-science-pipelines.adoc[Enabling data science pipelines].

Otherwise, skip to :creating-a-workbench.adoc[Creating a workbench].
