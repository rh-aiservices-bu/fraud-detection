{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install onnx==1.12.0 \\\n",
    "        onnxruntime==1.16.1 \\\n",
    "        seaborn==0.13.0 \\\n",
    "        tf2onnx==1.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import those dependencies we need to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV data which we will use to train the model.\n",
    "It contains the following fields:\n",
    "* **distancefromhome** - The distance from home where the transaction happened.\n",
    "* **distancefromlast_transaction** - The distance from last transaction happened.\n",
    "* **ratiotomedianpurchaseprice** - Ratio of purchased price compared to median purchase price.\n",
    "* **repeat_retailer** - If it's from a retailer that already has been purchased from before.\n",
    "* **used_chip** - If the (credit card) chip was used.\n",
    "* **usedpinnumber** - If the PIN number was used.\n",
    "* **online_order** - If it was an online order.\n",
    "* **fraud** - If the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('data/card_transdata.csv')\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data.\n",
    "# The only output data we have is if it's fraudulent or not, and all other fields go as inputs to the model.\n",
    "\n",
    "X = Data.drop(columns = ['repeat_retailer','distance_from_home', 'fraud'])\n",
    "y = Data['fraud']\n",
    "\n",
    "# Split the data into training and testing sets so we have something to test the trained model with.\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train)\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. This means that the data will be between -1 and 1, which makes it a lot easier for the model to learn than random potentially large values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "Path(\"artifact\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"artifact/test_data.pkl\", \"wb\") as handle:\n",
    "    pickle.dump((X_test, y_test), handle)\n",
    "with open(\"artifact/scaler.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), we set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "The model we build here is a simple fully connected deep neural network, containing 3 hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = len(X.columns)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get performance\n",
    "import os\n",
    "\n",
    "epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=epochs, \\\n",
    "                    validation_data=(scaler.transform(X_val.values),y_val), \\\n",
    "                    verbose = True, class_weight = class_weights)\n",
    "print(\"Training of model is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as ONNX for easy use of ModelMesh\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "os.makedirs(\"models/fraud\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confirm the model file has been created successfully\n",
    "* This should display the model file, with its size and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -alRh ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also create a date-stamped folder as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date-stamped folder for fraud models\n",
    "current_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "fraud_folder = os.path.join(\"models/\", current_date + \"-fraud\")\n",
    "os.makedirs(fraud_folder, exist_ok=True)\n",
    "\n",
    "# Save the model to the date-stamped folder\n",
    "model_path = os.path.join(fraud_folder, \"model.onnx\")\n",
    "onnx.save(model_proto, model_path)\n",
    "\n",
    "print(f\"Saved the model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Confirm the model file has been created successfully\n",
    "* This should display the model file(s), with its size and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -alh ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('artifact/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "with open('artifact/test_data.pkl', 'rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a onnx inference runtime session and predict values for all test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/fraud/model.onnx\", providers=rt.get_available_providers())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], {input_name: scaler.transform(X_test.values).astype(np.float32)})\n",
    "y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n",
    "threshold = 0.995\n",
    "y_pred = np.where(y_pred_temp > threshold, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(np.asarray(y_test) == y_pred) / len(y_pred)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "c_matrix = confusion_matrix(y_test,y_pred)\n",
    "ax = sns.heatmap(c_matrix, annot=True,fmt='d', cbar=False, cmap='Blues')\n",
    "ax.set_xlabel(\"Prediction\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with Sally's details\n",
    "\n",
    "Fields are in order: \n",
    "* distance_from_last_transaction\n",
    "* ratio_to_median_price\n",
    "* used_chip \n",
    "* used_pin_number\n",
    "* online_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sally_transaction_details = [\n",
    "    [0.3111400080477545,\n",
    "    1.9459399775518593,\n",
    "    1.0,\n",
    "    0.0,\n",
    "    0.0]\n",
    "    ]\n",
    "\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(sally_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Was Sally's transaction predicted to be fraudulent?  \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely to be fraudulent was it?  \")\n",
    "print(\"{:.5f}\".format(np.squeeze(prediction)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "63462a1f26ab486248b2a0fd058a0d9f9a6566a80083a3e1eb8f35617f2381b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
